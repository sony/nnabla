{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNabla Python API Demonstration Tutorial\n",
    "\n",
    "Let us import nnabla first, and some additional useful tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# python2/3 compatibility\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnabla as nn  # Abbreviate as nn for convenience.\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NdArray\n",
    "\n",
    "NdArray is a data container of a multi-dimensional array. NdArray is device (e.g. CPU, CUDA) and type (e.g. uint8, float32) agnostic, in which both type and device are implicitly casted or transferred when it is used. Below, you create a NdArray with a shape of `(2, 3, 4)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = nn.NdArray((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the values held inside `a` by the following. The values are not initialized, and are created as float32 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accessor `.data` returns a reference to the values of NdArray as `numpy.ndarray`. You can modify these by using the Numpy API as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[Substituting random values]')\n",
    "a.data = np.random.randn(*a.shape)\n",
    "print(a.data)\n",
    "print('[Slicing]')\n",
    "a.data[0, :, ::2] = 0\n",
    "print(a.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above operation is all done in the host device (CPU). NdArray provides more efficient functions in case you want to fill all values with a constant, `.zero` and `.fill`. They are lazily evaluated when the data is requested (when neural network computation requests the data, or when numpy array is requested by Python) The filling operation is executed within a specific device (e.g. CUDA GPU), and more efficient if you specify the device setting, which we explain later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fill(1)  # Filling all values with one.\n",
    "print(a.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create an NdArray instance directly from a Numpy array object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nn.NdArray.from_numpy_array(np.ones(a.shape))\n",
    "print(b.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NdArray is used in Variable class, as well as NNabla's imperative computation of neural networks. We describe them in the later sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable class is used when you construct a neural network. The neural network can be described as a graph in which an edge represents a function (a.k.a operator and layer) which defines operation of a minimum unit of computation, and a node represents a variable which holds input/output values of a function (Function class is explained later). The graph is called \"Computation Graph\".\n",
    "\n",
    "In NNabla, a Variable, a node of a computation graph, holds two `NdArray`s, one for storing the input or output values  of a function during forward propagation (executing computation graph in the forward order), while another for storing the backward error signal (gradient) during backward propagation (executing computation graph in backward order to propagate error signals down to parameters (weights) of neural networks). The first one is called `data`, the second is `grad` in NNabla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line creates a Variable instance with a shape of (2, 3, 4). It has `data` and `grad` as `NdArray`. The flag `need_grad` is used to omit unnecessary gradient computation during backprop if set to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Variable([2, 3, 4], need_grad=True)\n",
    "print('x.data:', x.data)\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the shape by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since both `data` and `grad` are `NdArray`, you can get a reference to its values as NdArray with the `.data` accessor, but also it can be referred by `.d` or `.g` property for `data` and `grad` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('x.data')\n",
    "print(x.d)\n",
    "x.d = 1.2345  # To avoid NaN\n",
    "assert np.all(x.d == x.data.data), 'd: {} != {}'.format(x.d, x.data.data)\n",
    "print('x.grad')\n",
    "print(x.g)\n",
    "x.g = 1.2345  # To avoid NaN\n",
    "assert np.all(x.g == x.grad.data), 'g: {} != {}'.format(x.g, x.grad.data)\n",
    "\n",
    "# Zeroing grad values\n",
    "x.grad.zero()\n",
    "print('x.grad (after `.zero()`)')\n",
    "print(x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `NdArray`, a `Variable` can also be created from Numpy array(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = nn.Variable.from_numpy_array(np.ones((3,)), need_grad=True)\n",
    "print(x2)\n",
    "print(x2.d)\n",
    "x3 = nn.Variable.from_numpy_array(np.ones((3,)), np.zeros((3,)), need_grad=True)\n",
    "print(x3)\n",
    "print(x3.d)\n",
    "print(x3.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides storing values of a computation graph, pointing a parent edge (function) to trace the computation graph is an important role. Here `x` doesn't have any connection. Therefore, the `.parent` property returns None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function defines a operation block of a computation graph as we described above. The module `nnabla.functions` offers various functions (e.g. Convolution, Affine and ReLU). You can see the list of functions available in the [API reference guide](http://nnabla.readthedocs.io/en/latest/python/api/function.html#module-nnabla.functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nnabla.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, here you will defines a computation graph that computes the element-wise Sigmoid function outputs for the input variable and sums up all values into a scalar. (This is simple enough to explain how it behaves but a meaningless example in the context of neural network training. We will show you a neural network example later.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigmoid_output = F.sigmoid(x)\n",
    "sum_output = F.reduce_sum(sigmoid_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function API in `nnabla.functions` takes one (or several) Variable(s) and arguments (if any), and returns one (or several) output Variable(s). The `.parent` points to the function instance which created it.\n",
    "Note that no computation occurs at this time since we just define the graph. (This is the default behavior of NNabla computation graph API. You can also fire actual computation during graph definition which we call \"Dynamic mode\" (explained later))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sigmoid_output.parent.name:\", sigmoid_output.parent.name)\n",
    "print(\"x:\", x)\n",
    "print(\"sigmoid_output.parent.inputs refers to x:\", sigmoid_output.parent.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum_output.parent.name:\", sum_output.parent.name)\n",
    "print(\"sigmoid_output:\", sigmoid_output)\n",
    "print(\"sum_output.parent.inputs refers to sigmoid_output:\", sum_output.parent.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.forward()` at a leaf Variable executes the forward pass computation in the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_output.forward()\n",
    "print(\"CG output:\", sum_output.d)\n",
    "print(\"Reference:\", np.sum(1.0 / (1.0 + np.exp(-x.d))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `.backward()` does the backward propagation through the graph. Here we initialize the `grad` values as zero before backprop since the NNabla backprop algorithm always accumulates the gradient in the root variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.zero()\n",
    "sum_output.backward()\n",
    "print(\"d sum_o / d sigmoid_o:\")\n",
    "print(sigmoid_output.g)\n",
    "print(\"d sum_o / d x:\")\n",
    "print(x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNabla is developed by mainly focused on neural network training and inference. Neural networks have parameters to be learned associated with computation blocks such as Convolution, Affine (a.k.a. fully connected, dense etc.). In NNabla, the learnable parameters are also represented  as `Variable` objects. Just like input variables, those parameter variables are also used by passing into `Function`s. For example, Affine function takes input, weights and biases as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = nn.Variable([5, 2])  # Input\n",
    "w = nn.Variable([2, 3], need_grad=True)  # Weights\n",
    "b = nn.Variable([3], need_grad=True)  # Biases\n",
    "affine_out = F.affine(x, w, b)  # Create a graph including only affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example takes an input with B=5 (batchsize) and D=2 (dimensions) and maps it to D'=3 outputs, i.e. (B, D') output.\n",
    "\n",
    "You may also notice that here you set `need_grad=True` only for parameter variables (w and b). The x is a non-parameter variable and the root of computation graph. Therefore, it doesn't require gradient computation. In this configuration, the gradient computation for x is not executed in the first affine, which will omit the computation of unnecessary backpropagation.\n",
    "\n",
    "The next block sets data and initializes grad, then applies forward and backward computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random input and parameters\n",
    "x.d = np.random.randn(*x.shape)\n",
    "w.d = np.random.randn(*w.shape)\n",
    "b.d = np.random.randn(*b.shape)\n",
    "# Initialize grad\n",
    "x.grad.zero()  # Just for showing gradients are not computed when need_grad=False (default).\n",
    "w.grad.zero()\n",
    "b.grad.zero()\n",
    "\n",
    "# Forward and backward\n",
    "affine_out.forward()\n",
    "affine_out.backward()\n",
    "# Note: Calling backward at non-scalar Variable propagates 1 as error message from all element of outputs. ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that affine_out holds an output of Affine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F.affine')\n",
    "print(affine_out.d)\n",
    "print('Reference')\n",
    "print(np.dot(x.d, w.d) + b.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting gradients of weights and biases are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"dw\")\n",
    "print(w.g)\n",
    "print(\"db\")\n",
    "print(b.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of `x` is not changed because `need_grad` is set as False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametric Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering parameters as inputs of `Function` enhances expressiveness and flexibility of computation graphs.\n",
    "However, to define all parameters for each learnable function is annoying for users to define a neural network.\n",
    "In NNabla, trainable models are usually created by composing functions that have optimizable parameters.\n",
    "These functions are called \"Parametric Functions\".\n",
    "The Parametric Function API provides various parametric functions and an interface for composing trainable models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use parametric functions, import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nnabla.parametric_functions as PF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function with optimizable parameter can be created as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nn.parameter_scope(\"affine1\"):\n",
    "    c1 = PF.affine(x, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line creates a **parameter scope**. The second line then applies `PF.affine` - an affine transform - to `x`, and creates a variable `c1` holding that result. The parameters are created and initialized randomly at function call, and registered by a name \"affine1\" using `parameter_scope` context. The function `nnabla.get_parameters()` allows to get the registered parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `name=` argument of any PF function creates the equivalent parameter space to the above definition of `PF.affine` transformation as below. It could save the space of your Python code. The `nnabla.parametric_scope` is more useful when you group multiple parametric functions such as Convolution-BatchNormalization found in a typical unit of CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = PF.affine(x, 3, name='affine1')\n",
    "nn.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the shapes of both outputs and parameter variables (as you can see above) are automatically determined by only providing the output size of affine transformation(in the example above the output size is 3). This helps to create a graph in an easy way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter scope can be nested as follows (although a meaningless example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with nn.parameter_scope('foo'):\n",
    "    h = PF.affine(x, 3)\n",
    "    with nn.parameter_scope('bar'):\n",
    "        h = PF.affine(h, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, `get_parameters()` can be used in `parameter_scope`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nn.parameter_scope(\"foo\"):\n",
    "    print(nn.get_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nnabla.clear_parameters()` can be used to delete registered parameters under the scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nn.parameter_scope(\"foo\"):\n",
    "    nn.clear_parameters()\n",
    "print(nn.get_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Example For Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block creates a computation graph to predict one dimensional output from two dimensional inputs by a 2 layer fully connected neural network (multi-layer perceptron)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.clear_parameters()\n",
    "batchsize = 16\n",
    "x = nn.Variable([batchsize, 2])\n",
    "with nn.parameter_scope(\"fc1\"):\n",
    "    h = F.tanh(PF.affine(x, 512))\n",
    "with nn.parameter_scope(\"fc2\"):\n",
    "    y = PF.affine(h, 1)\n",
    "print(\"Shapes:\", h.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create the following parameter variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.get_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described above, you can execute the forward pass by calling forward method at the terminal variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.d = np.random.randn(*x.shape)  # Set random input\n",
    "y.forward()\n",
    "print(y.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural networks needs a loss value to be minimized by gradient descent with backprop. In NNabla, loss function is also a just function, and packaged in the functions module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for label\n",
    "label = nn.Variable([batchsize, 1])\n",
    "# Set loss\n",
    "loss = F.reduce_mean(F.squared_error(y, label))\n",
    "\n",
    "# Execute forward pass.\n",
    "label.d = np.random.randn(*label.shape)  # Randomly generate labels\n",
    "loss.forward()\n",
    "print(loss.d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've seen above, NNabla `backward` accumulates the gradients at the root variables. You have to initialize the grad of the parameter variables before backprop (We will show you the easiest way with `Solver` API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collect all parameter variables and init grad.\n",
    "for name, param in nn.get_parameters().items():\n",
    "    param.grad.zero()\n",
    "# Gradients are accumulated to grad of params.\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imperative Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing backprop, gradients are held in parameter variable grads. The next block will update the parameters with vanilla gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for name, param in nn.get_parameters().items():\n",
    "    param.data -= param.grad * 0.001  # 0.001 as learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above computation is an example of NNabla's \"Imperative Mode\" for executing neural networks. Normally, NNabla functions (instances of [nnabla.functions](https://nnabla.readthedocs.io/en/latest/python/api/function.html#module-nnabla.functions)) take `Variable`s as their input. When at least one `NdArray` is provided as an input for NNabla functions (instead of `Variable`s), the function computation will be fired immediately, and returns an `NdArray` as the output, instead of returning a `Variable`. In the above example, the NNabla functions `F.mul_scalar` and `F.sub2` are called by the overridden operators `*` and `-=`, respectively.\n",
    "\n",
    "In other words, NNabla's \"Imperative mode\" doesn't create a computation graph, and can be used like NumPy. If device acceleration such as CUDA is enabled, it can be used like NumPy empowered with device acceleration. Parametric functions can also be used with NdArray input(s). The following block demonstrates a simple imperative execution example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A simple example of imperative mode.\n",
    "xi = nn.NdArray.from_numpy_array(np.arange(4).reshape(2, 2))\n",
    "yi = F.relu(xi - 1)\n",
    "print(xi.data)\n",
    "print(yi.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in-place substitution from the rhs to the lhs cannot be done by the `=` operator. For example, when `x` is an `NdArray`, writing `x = x + 1` will *not* increment all values of `x` - instead, the expression on the lhs will create a *new* `NdArray` object that different from the one originally bound by `x`, and binds the new `NdArray` object to the Python variable `x` on the rhs.\n",
    "\n",
    "For in-place editing of `NdArrays`, the in-place assignment operators `+=`, `-=`, `*=`, and `/=` can be used. The `copy_from` method can also be used to copy values of an existing `NdArray` to another. For example, incrementing 1 to `x`, an `NdArray`, can be done by `x.copy_from(x+1)`. The copy is performed with device acceleration if a device context is specified by using `nnabla.set_default_context` or `nnabla.context_scope`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following doesn't perform substitution but assigns a new NdArray object to `xi`. \n",
    "# xi = xi + 1\n",
    "\n",
    "# The following copies the result of `xi + 1` to `xi`.\n",
    "xi.copy_from(xi + 1)\n",
    "assert np.all(xi.data == (np.arange(4).reshape(2, 2) + 1))\n",
    "\n",
    "# Inplace operations like `+=`, `*=` can also be used (more efficient).\n",
    "xi += 1\n",
    "assert np.all(xi.data == (np.arange(4).reshape(2, 2) + 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNabla provides stochastic gradient descent algorithms to optimize parameters listed in the `nnabla.solvers` module. The parameter updates demonstrated above can be replaced with this Solver API, which is easier and usually faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nnabla import solvers as S\n",
    "solver = S.Sgd(lr=0.00001)\n",
    "solver.set_parameters(nn.get_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set random data\n",
    "x.d = np.random.randn(*x.shape)\n",
    "label.d = np.random.randn(*label.shape)\n",
    "\n",
    "# Forward\n",
    "loss.forward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just call the the following solver method to fill zero grad region, then backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver.zero_grad()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block updates parameters with the Vanilla Sgd rule (equivalent to the imperative example above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solver.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Problem To Demonstrate Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function defines a regression problem which computes the norm of a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vector2length(x):\n",
    "    # x : [B, 2] where B is number of samples.\n",
    "    return np.sqrt(np.sum(x ** 2, axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize this mapping with the contour plot by matplotlib as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for plotting contour on a grid data.\n",
    "xs = np.linspace(-1, 1, 100)\n",
    "ys = np.linspace(-1, 1, 100)\n",
    "grid = np.meshgrid(xs, ys)\n",
    "X = grid[0].flatten()\n",
    "Y = grid[1].flatten()\n",
    "\n",
    "def plot_true():\n",
    "    \"\"\"Plotting contour of true mapping from a grid data created above.\"\"\"\n",
    "    plt.contourf(xs, ys, vector2length(np.hstack([X[:, None], Y[:, None]])).reshape(100, 100))\n",
    "    plt.axis('equal')\n",
    "    plt.colorbar()\n",
    "    \n",
    "plot_true()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a deep prediction neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def length_mlp(x):\n",
    "    h = x\n",
    "    for i, hnum in enumerate([4, 8, 4, 2]):\n",
    "        h = F.tanh(PF.affine(h, hnum, name=\"fc{}\".format(i)))\n",
    "    y = PF.affine(h, 1, name='fc')\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.clear_parameters()\n",
    "batchsize = 100\n",
    "x = nn.Variable([batchsize, 2])\n",
    "y = length_mlp(x)\n",
    "label = nn.Variable([batchsize, 1])\n",
    "loss = F.reduce_mean(F.squared_error(y, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a 5 layers deep MLP using for-loop. Note that only 3 lines of the code potentially create infinitely deep neural networks. The next block adds helper functions to visualize the learned function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(inp):\n",
    "    ret = []\n",
    "    for i in range(0, inp.shape[0], x.shape[0]):\n",
    "        xx = inp[i:i + x.shape[0]]\n",
    "        # Imperative execution\n",
    "        xi = nn.NdArray.from_numpy_array(xx)\n",
    "        yi = length_mlp(xi)\n",
    "        ret.append(yi.data.copy())\n",
    "    return np.vstack(ret)\n",
    "\n",
    "def plot_prediction():\n",
    "    plt.contourf(xs, ys, predict(np.hstack([X[:, None], Y[:, None]])).reshape(100, 100))\n",
    "    plt.colorbar()\n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we instantiate a solver object as follows. We use Adam optimizer which is one of the most popular SGD algorithm used in the literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nnabla import solvers as S\n",
    "solver = S.Adam(alpha=0.01)\n",
    "solver.set_parameters(nn.get_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function generates data from the true system infinitely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_data_provider(n):\n",
    "    x = np.random.uniform(-1, 1, size=(n, 2))\n",
    "    y = vector2length(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next block, we run 2000 training steps (SGD updates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 2000\n",
    "for i in range(num_iter):\n",
    "    # Sample data and set them to input variables of training. \n",
    "    xx, ll = random_data_provider(batchsize)\n",
    "    x.d = xx\n",
    "    label.d = ll\n",
    "    # Forward propagation given inputs.\n",
    "    loss.forward(clear_no_need_grad=True)\n",
    "    # Parameter gradients initialization and gradients computation by backprop.\n",
    "    solver.zero_grad()\n",
    "    loss.backward(clear_buffer=True)\n",
    "    # Apply weight decay and update by Adam rule.\n",
    "    solver.weight_decay(1e-6)\n",
    "    solver.update()\n",
    "    # Just print progress.\n",
    "    if i % 100 == 0 or i == num_iter - 1:\n",
    "        print(\"Loss@{:4d}: {}\".format(i, loss.d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory usage optimization**: You may notice that, in the above updates, `.forward()` is called with the `clear_no_need_grad=` option, and `.backward()` is called with the `clear_buffer=` option. Training of neural network in more realistic scenarios usually consumes huge memory due to the nature of backpropagation algorithm, in which all of the forward variable buffer `data` should be kept in order to compute the gradient of a function. In a naive implementation, we keep all the variable `data` and `grad` living until the `NdArray` objects are not referenced (i.e. the graph is deleted). The `clear_*` options in `.forward()` and `.backward()` enables to save memory consumption due to that by clearing (erasing) memory of `data` and `grad` when it is not referenced by any subsequent computation. (More precisely speaking, it doesn't free memory actually. We use our memory pool engine by default to avoid memory alloc/free overhead). The unreferenced buffers can be re-used in subsequent computation. See the document of `Variable` for more details. Note that the following `loss.forward(clear_buffer=True)` clears `data` of any intermediate variables. If you are interested in intermediate variables for some purposes (e.g. debug, log), you can use the `.persistent` flag to prevent clearing buffer of a specific `Variable` like below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.forward(clear_buffer=True)\n",
    "print(\"The prediction `y` is cleared because it's an intermediate variable.\")\n",
    "print(y.d.flatten()[:4])  # to save space show only 4 values\n",
    "y.persistent = True\n",
    "loss.forward(clear_buffer=True)\n",
    "print(\"The prediction `y` is kept by the persistent flag.\")\n",
    "print(y.d.flatten()[:4])  # to save space show only 4 value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm the prediction performs fairly well by looking at the following visualization of the ground truth and prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.title(\"Ground truth\")\n",
    "plot_true()\n",
    "plt.subplot(122)\n",
    "plt.title(\"Prediction\")\n",
    "plot_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save learned parameters by `nnabla.save_parameters` and load by `nnabla.load_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_param = \"param-vector2length.h5\"\n",
    "nn.save_parameters(path_param)\n",
    "# Remove all once\n",
    "nn.clear_parameters()\n",
    "nn.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load again\n",
    "nn.load_parameters(path_param)\n",
    "print('\\n'.join(map(str, nn.get_parameters().items())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both save and load functions can also be used in a parameter scope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with nn.parameter_scope('foo'):\n",
    "    nn.load_parameters(path_param)\n",
    "print('\\n'.join(map(str, nn.get_parameters().items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm {path_param}  # Clean ups"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

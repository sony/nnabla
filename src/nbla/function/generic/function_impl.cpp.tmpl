// Copyright (c) 2017 Sony Corporation. All Rights Reserved.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.


#include <nbla/array.hpp>
#include <nbla/common.hpp>
#include <nbla/function/${snake_name}.hpp>
#include <nbla/variable.hpp>

// TODO: remove the following headers if not used.
#include <iostream>
#include <typeinfo>

<%
from utils.type_conv import type_from_proto
dec_targs = ', '.join(['typename ' + t for t in ttypes])
targs = ', '.join(ttypes)
%>

namespace nbla {

NBLA_REGISTER_FUNCTION_SOURCE(${', '.join([name] + [type_from_proto[v['type']]['cpp'] for v in arguments.values()])});

template <${dec_targs}>
void ${name}<${targs}>::setup_impl(const Variables &inputs,
                               const Variables &outputs) {
  // TODO: Remove debug message
  std::cout << "${name}<" << typeid(T).name()
            << ">::setup_impl called with " << this->ctx_.to_string() << "."
            << std::endl;
  // TODO: reshape outputs
% for i, (oname, o) in enumerate(outputs.items()):
  // outputs[${i}]->reshape({}, true);
% endfor

  /* TODO: Any preparation comes here.
     Note that, although it is called only when a computation graph is
     constructed in a static computation graph, in a dynamic computation graph,
     it's called every time. Keep the setup computation light for the performance
     (caching heavy computation, device synchronization in GPU etc.)
  */
}

template <${dec_targs}>
void ${name}<${targs}>::forward_impl(const Variables &inputs,
                               const Variables &outputs) {
  // TODO: Remove debug message
  std::cout << "${name}<" << typeid(T).name()
            << ">::forward_impl called with " << this->ctx_.to_string() << "."
            << std::endl;

  /* TODO: remove this help message.
    The type `Variables` is a typedef of `vector<Variable*>`.
    The `Variable` class owns storage of data (storage for forward propagation)
    and grad (for backprop) respectively.
    
    You can get a raw pointer of a scalar type of the storage using:

    - `const T* Variable::get_{data|grad}_pointer<T>(ctx)` for read-only access.
    - `T* Variable::cast_{data|grad}_and_get_pointer<T>(ctx)` for r/w access.

    By this, automatic type conversion would occur if data was held in a different type.
  */
  // Inputs
% for i, (vin_name, vin) in enumerate(inputs.items()):
% if vin.get('optional', false):
  const ${in_types[i]}* ${vin_name}{nullptr};
  if (inputs.size() > ${i}) {
    ${vin_name} = inputs[${i}]->get_data_pointer<${in_types[i]}>(this->ctx_);
  }
% else:
  const ${in_types[i]}* ${vin_name} = inputs[${i}]->get_data_pointer<${in_types[i]}>(this->ctx_);
% endif
% endfor

  // Outputs
  /* TODO: remove this help message.
     Array instances of output variables are retrieved with a write-only flag
     (second argument) in order to avoid an unnecessary copy from another
     array instance.
  */
% for i, (vout_name, vout) in enumerate(outputs.items()):
% if vout.get('optional', false):
  ${out_types[i]}* ${vout_name}{nullptr};
  if (outputs.size() > ${i}) {
    ${vout_name} = outputs[${i}]->cast_data_and_get_pointer<${out_types[i]}>(this->ctx_, true);
  }
% else:
  ${out_types[i]}* ${vout_name} = outputs[${i}]->cast_data_and_get_pointer<${out_types[i]}>(this->ctx_, true);
% endif
% endfor

  // TODO: Write implementation
}


template <${dec_targs}>
void ${name}<${targs}>::backward_impl(const Variables &inputs,
                                const Variables &outputs,
                                const vector<bool> &propagate_down,
                                const vector<bool> &accum) {
  // TODO: Remove debug message
  std::cout << "${name}<" << typeid(T).name()
            << ">::backward_impl called with " << this->ctx_.to_string() << "."
            << std::endl;
<%
pp = []
for i, (vin_name, vin) in enumerate(inputs.items()):
  if vin.get('optional', False):
    pp.append('(inputs.size() > {0} && propagate_down[{0}])'.format(i))
  else:
    pp.append('propagate_down[{}]'.format(i))
pp = '!(%s)' % ' || '.join(pp)
%>
  /* TODO: remove this help message.
     The propagate down flags are automatically set by our graph engine, which
     specifies whether each input variable of them requires gradient
     computation. 
  */
  if (${pp}) {
    return;
  }

  /** TODO: remove this help message.
      The backward error signals are propagated through the graph, and the
      error from descendant functions are set in the grad region of the output variables.
   */
  // Gradient of outputs
% for i, (vout_name, vout) in enumerate(outputs.items()):
% if vout.get('optional', false):
  const ${out_types[i]}* g_${vout_name}{nullptr};
  if (outputs.size() > ${i}) {
    g_${vout_name} = outputs[${i}]->get_grad_pointer<${out_types[i]}>(this->ctx_);
  }
% else:
  const ${out_types[i]}* g_${vout_name} = outputs[${i}]->get_grad_pointer<${out_types[i]}>(this->ctx_);
% endif
% endfor

  /* TODO: remove this help message.
     The backward error signal should be propagated to the grad region of input
     variables.

     The accum flags are also set by our graph engine, which specifies whether
     each input variable of them wants the result of the gradient computation
     by substitution or accumulation.
  */
  // Gradient of inputs
  /* TODO: remove this help message.
     Array instances of gradients of inputs are transferred with write-only
     flag when accum options are false.
  */
% for i, (vin_name, vin) in enumerate(inputs.items()):
  ${in_types[i]}* g_${vin_name}{nullptr};
% endfor

% for i, (vin_name, vin) in enumerate(inputs.items()):
% if vin.get('optional', false):
  if (inputs.size() > ${i} && propagate_down[${i}]) {
    g_${vin_name} = inputs[${i}]->cast_grad_and_get_pointer<${in_types[i]}>(this->ctx_, !accum[${i}]);
    // TODO: Write gradient computation of ${vin_name}
  }
% else:
  if (propagate_down[${i}]) {
    g_${vin_name} = inputs[${i}]->cast_grad_and_get_pointer<${in_types[i]}>(this->ctx_, !accum[${i}]);
    // TODO: Write gradient computation of ${vin_name}
  }
% endif
% endfor
}
}
